{"name":"Pml barbell","tagline":"Submission for barbell project in Practical Machine Learning Coursera Course","body":"# Practical Machine Learning Course Submission\r\n\r\nThe processing steps performed in the uploaded R script are summarised below. The code itself is also reasonably well commented.\r\n\r\n## 1) Loading data\r\n\r\n## 2) Pre-Processing\r\n\r\nOnly data columns (attributes) which provide some differentiation in the final test data-set are kept.\r\nThis could be considered 'cheating' as the final test data is examined; however it is reasonable to assume that in most applications where a prediction is necessary which attributes will be gathered going forwards would be known.\r\n\r\nAll of the attributes in the final test set are converted to factors; and those with fewer than 2 factors (so no differentiation between any of the samples) are dropped from the data.\r\n\r\n## 3) Data Splitting\r\n\r\nThe training data is split into 3 sets, with 70% of the observations used for model training, 20% used for cross-validation and any required model tuning work, and 10% used for making a final estimation of the model accuracy.\r\n\r\n## 4) Model Training\r\n\r\nThe model is then trained using the random forest method (method='rf') from within the caret package. All of the default values are used; as an initial attempt, and all attributes (i.e. all data columns other than the class) were included as predictors.\r\nThis training step takes rather a long time to run.\r\n\r\n## 5) Checking Accuracy\r\n\r\nThe accuracy of the model was then checked against the training and cross-validation sets. These both gave a very low miss-classification error. The confusion matrices are reproduced below for reference:\r\n\r\n***\r\n\r\n    Confusion Matrix and Statistics (Training Set)\r\n               Reference\r\n     Prediction    A    B    C    D    E\r\n              A 3906    0    0    0    0\r\n              B    0 2658    0    0    0\r\n              C    0    0 2395    0    0\r\n              D    0    0    0 2251    0\r\n              E    0    0    0    0 2525\r\n\r\n    Overall Statistics\r\n\r\n                    Accuracy : 1          \r\n                      95% CI : (0.9997, 1)\r\n         No Information Rate : 0.2844     \r\n         P-Value [Acc > NIR] : < 2.2e-16  \r\n\r\n***\r\n\r\n    Confusion Matrix and Statistics (Validation Set)\r\n    \r\n              Reference\r\n    Prediction    A    B    C    D    E\r\n             A 1116    0    0    0    0\r\n             B    0  760    0    0    0\r\n             C    0    0  685    0    0\r\n             D    0    0    0  644    0\r\n             E    0    0    0    0  722\r\n    \r\n    Overall Statistics\r\n                                         \r\n                   Accuracy : 1          \r\n                     95% CI : (0.9991, 1)\r\n        No Information Rate : 0.2842     \r\n        P-Value [Acc > NIR] : < 2.2e-16  \r\n \r\n\r\n***\r\n\r\nGiven how low the miss-classification error was it was decided that no further model tuning was required, so all that was left is to make an estimate of the out of sample error.\r\nThis is done by attempting to predict the class of the test instances. The confusion matrix for the test set is given below. This would suggest the out-of-sample accuracy is exactly 1.\r\n\r\n***\r\n\r\n    Confusion Matrix and Statistics (actual test set)\r\n    \r\n              Reference\r\n    Prediction   A   B   C   D   E\r\n             A 558   0   0   0   0\r\n             B   0 379   0   0   0\r\n             C   0   0 342   0   0\r\n             D   0   0   0 321   0\r\n             E   0   0   0   0 360\r\n    \r\n    Overall Statistics\r\n                                         \r\n                   Accuracy : 1          \r\n                     95% CI : (0.9981, 1)\r\n        No Information Rate : 0.2847     \r\n        P-Value [Acc > NIR] : < 2.2e-16  \r\n\r\n***\r\n\r\nBecause no modifications were made to the base random forest method, it is likely that a slightly better model could be made by running the training method again on all of the data; but it would not then be possible to quote an expected out of sample accuracy.\r\n\r\n## 6) Make Predictions\r\n\r\nFinally predictions are made regarding the actual test set (for which the actual classification is not known).\r\nRather strangely this predicted that all 20 of the test cases were in class 'A', which seems a little odd, but this was the answer submitted.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}